program: llama-tune.py
method: bayes
metric:
  name: eval_accuracy
  goal: maximize
parameters:
  model:
    values: ["llama2"]
  epochs:
    min: 3
    max: 5
  batch_size:
    values: [4, 8, 16]
  learning_rate:
    values: [1e-5, 5e-5, 1e-3]
  lora_r:
    values: [4, 8, 12]
  lora_alpha:
    values: [16, 32, 64]
  lora_dropout:
    values: [0.05, 0.1]
