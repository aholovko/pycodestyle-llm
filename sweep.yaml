program: llama-tune.py
method: bayes
metric:
  name: eval_accuracy
  goal: maximize
parameters:
  model:
    values: ["llama2", "llama3"]
  epochs:
    values: [3, 4, 5]
  batch_size:
    values: [4, 8, 16]
  learning_rate:
    distribution: "log_uniform_values"
    min: 0.0001
    max: 0.1
  lora_r:
    values: [4, 8, 12]
  lora_alpha:
    values: [16, 32, 64]
  lora_dropout:
    values: [0.05, 0.1]
