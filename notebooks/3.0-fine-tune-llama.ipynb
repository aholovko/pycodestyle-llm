{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning Llama models",
   "id": "387d7a7bbd0218a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset",
   "id": "4972d09cfbac698e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "indent_ds = load_dataset(\"aholovko/pep8_indentation_compliance\")\n",
    "indent_ds"
   ],
   "id": "582ad7e1c112e9dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "indent_df = indent_ds[\"train\"].to_pandas()\n",
    "indent_df.head()"
   ],
   "id": "16e13b08c3cbf67d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features = indent_ds[\"train\"].features\n",
    "features"
   ],
   "id": "96e855c043053fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features[\"label\"].int2str(0)",
   "id": "779845028e0f3ddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "id2label = {idx: features[\"label\"].int2str(idx) for idx in range(2)}\n",
    "id2label"
   ],
   "id": "3e57fd298ff59f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label2id = {v:k for k,v in id2label.items()}\n",
    "label2id"
   ],
   "id": "72cbc1c4214f2a71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "indent_df[\"label\"].value_counts(normalize=True).sort_index()",
   "id": "c1e690493e051a3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization",
   "id": "4e934bcd748c4223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# tokenizer(indent_ds[\"train\"][\"code\"][:1])"
   ],
   "id": "b7b61c96c99229dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples[\"code\"], truncation=True, max_length=512, padding='longest')"
   ],
   "id": "eba4f5d42df3d58a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "indent_ds = indent_ds.map(tokenize_text, batched=True)\n",
    "indent_ds"
   ],
   "id": "f8a60e18ec4a7149",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-tuning model",
   "id": "9c99f7a412386fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=False,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "# \n",
    "# print(torch.backends.mps.is_available())\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=2,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='mps',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n"
   ],
   "id": "9c2695b45d24b6b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "id": "81b000776e096965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "logging_steps = len(indent_ds[\"train\"]) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama2-tuned\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=indent_ds[\"train\"],\n",
    "    eval_dataset=indent_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "736e4713a6fefe06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
